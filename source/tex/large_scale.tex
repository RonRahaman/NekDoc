The largest simulations performed so far with Nek5000 are in the range of \(5\times 10^6\). 
Performance aspects to keep in mind
\begin{itemize}
\item design your SEM-mesh for a polynomial order N=7 or N=9 (lx1 in SIZE)
\item turn on dealiasing only if needed and try to minimize the polynomial order used for the fine grid (lxd in SIZE)
\item ensure that you have at least 50 elements per MPI process (the more the better)
\item explore the pressure solver performance using AMG solver (sweet spot depends on number of processors and elements)
\item make sure usrchk() does not contain time consuming operations getting called in the time loop
\item enable tuned MxM implementation for your platform (see makenek options)
\item turn on residual projection scheme (see .rea file parameters)
\item tune your pressure/velocity tolerances (e.g. use 5e-5 for pressure and 1e-8 for velocity solver) having in mind your overall accuracy
\item try to maximize the timestep, if needed turn on OIFS scheme with a target Courant number of around 2.0
\item use the parallel I/O (MPI-IO or custom kernel) to write checkpoints to disk
\item understand where you spend most of the time - turn on solver and MPI timings to monitor solver performance (see makenek options)
\end{itemize}



In such context it is recommendable to save the .rea file as a binary re2. 

Also for input/output it may be necessary to use MPI I/O. In this case the code has to be compiled with MPI I/O, i.e. the line PPLIST="MPIIO" in the makefile should not be commented. For output we may use iofiles, the parameter 65 in the .rea file specifies the number of directories and separate files that have to be created as specified by the user.

For large scale simulations the AMG solver is a better, faster choice for solving the Poisson problem (the default solver is XXT).

\subparagraph*{AMG solver}
The code should be compiled once with the settings \texttt{AMG=true}, \texttt{AMG\_ DUMP=true}. In the tools folder of Nek5000 we can find the AMG solver, a Matlab version for the moment which is subject to further integration in the main code. The user should run the script run which will read the AMG dump files and create new ones. The new files are now to be used in the code and with \texttt{AMG\_ DUMP} commented out the user should recompile and run his Nek5000 version.

The AMG solver is a 3 stage process. 

The first step will generate the files needed for the matlab code. Next matlab must run the setup code and generate a set of .dat files. Then nek can run with the .dat files and use the AMG pressure solver.

AMG dump stage

    Make sure {\tt IFAMG} and {\tt IFAMG\_ DUMP} in makenek are uncommented and set to true
    Run makenek clean, then makenek <casename>
    Run Nek (this will produce a set of *.dat files) 

MATLAB AMG stage

    Move the {\tt amgdmp\_ *.dat} files to {\tt nek5\_ svn/trunk/tools/amg\_ matlab}: 

    {\tt mv amgdmp*.dat ../../trunk/tools/amg\_ matlab}

    {\tt cd ../../trunk/tools/amg\_ matlab}

    Run the script: {\tt tools/amg\_ matlab/run} (this may take several hours and will produce set of files) 

AMG run stage

    Move all *.dat files produced back to your case directory: 

    {\tt mv *.dat /path/to/case/dir}

    Comment IFAMG\_ DUMP in makenek (IFAMG should still be set to TRUE)
    Run makenek clean, then run {\tt makenek <casename>}
    Run Nek (the generated AMG files will be read during the AMG setup phase) 

Notes on improving AMG results:

    To help speed up the matlab process, try running the 1st stage, the AMG dump stage, with {\tt lx1=3} in the SIZE file. Using a lower lx1 number will create a sparser matrix and thus a speedier matlab resolution. lx1 can be increased when ready to run the 2nd stage, the AMG run stage, after the .dat files are produced. 

    To increase accuracy in the AMG results, try tightening the tolerances in the run script, in {\tt trunk/tools/amg\_ matlab}. Specifically, the first tolerance (default set to 0.5). Lowering this (say, to 0.1), will increase the time the matlab code stage takes, but the result will be a faster convergence in the pressure solves of the AMG run stage. 

\subparagraph*{Size related issues}  

Large simulations require a high number of nodes and thus a high number of variables. So far Nek5000 supports by default 4 byte integers, however the node numbering for big meshes may exceed this size and 8 byte integers may be needed. How is this done?

If more than 9 passive scalars are needed

Exiting Nek5000 while a batch job in being executed should be done not using \texttt{"qdel"} but \texttt{echo -1 > ioinfo}. 

\subparagraph*{MAKENEK}
 The shell script makenek is designed to assist the compilation process of NEK5000. The script will create a makefile based on the user settings section in makenek. The GNU gmake utility is used to build NEK5000.
Available configurations options:
\begin{table}[tb!]
\caption[Makenek]
{Compiler options} 
\label{tab:bdms}
\begin{center}
\begin{tabularx}{\textwidth}{@{} c c c Y @{}} % use 'Y' for first column
 \hline\hline 
  name 	&values 	&default 	&description \\ \hline\hline 
  PPLIST &	string 	&&	list of pre-processor symbols (BG,MOAB,BLAS\_ MXM, MPIIO)  \\ \hline
IFMPI &	true,false &true &	use MPI (needed for a multiprocessor computation) \\ \hline
IFAMG\_ DUMP & 	true,false 	&false &	dump AMG pre-processing files  \\ \hline
IFAMG 	&true,false &	false &	use AMG as coarse grid solver for pressure preconditioner else XXT  \\ \hline
F77 	&string &	mandatory 	&Fortran compiler (e.g. MPI: mpif77)  \\ \hline
CC &	string &	mandatory 	&C compiler (e.g. MPI: mpicc)  \\ \hline
G& 	string 	&optional &	optional compilation flags  \\ \hline
OPT\_ FLAGS\_ STD7 &	string &	optional &	optimization flags for L1,L2,L3  \\ \hline
OPT\_ FLAGS\_ MAG &	string &	optional& 	optimization flags for L4 (highest opt level)  \\ \hline
SOURCE\_ ROOT &	string &	mandatory &	path of nek5000 source  \\ \hline
USR 	&string &	optional &	object list of additional files to compile (make intructions (makefile\_ usr.inc required)  \\ \hline
USR\_ LFLAGS &	string &	optional &	optional linking flags  \\ \hline
MOAB\_ DIR &	string 	&NEK with MOAB &	Path to MOAB directories  \\ \hline
IFVISIT 	&true,false &	false 	&Toggles Visit in situ. See Visit\_ in\_ situ for details  \\ \hline
VISIT\_ INSTALL &	string &VISIT in situ &	Path to VISIT install path. See Visit\_ in\_ situ for details.  \\ \hline
VISIT\_ STOP 	&true,false &	false 	&When running VISIT in situ, simulation stops after step 1 to connect VISIT.  \\
 \hline\hline
\end{tabularx}
\end{center}
\end{table}



\subparagraph*{Binary geometry}
 Reatore2
Jump to: navigation, search

The NEK5000 tool, reatore2 allows users to split an ASCII .rea file to an ASCII .rea and a binary .re2 file. The .re2 file contains the mesh and boundary condition data that is normally written in ASCII in the .rea file. For large simulations, this information can be substantial, so storing it in binary lowers the memory footprint for the simulation.
Running reatore2

    Be sure that your nekton tools are up-to-date and compiled.
    At the command prompt type: reatore2 

NOTE-If the executables for the tools were not placed in the bin directory(default), 
include the path to the reatore2 executable

    User is prompted for name of .rea file 

    -Enter the name to the .rea file, excluding the .rea extenstion 

    User is prompted for the new files name 

    -Enter the name for your new files 


